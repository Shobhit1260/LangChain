
## ðŸ¤– What is a Prompt?

* A **prompt** is the instruction or question sent to an LLM
* LLM output is **highly sensitive** to how a prompt is written
* Small changes in prompts can lead to very different results
* This sensitivity led to **Prompt Engineering** as a skill/field

---

## ðŸŒ¡ï¸ LLM Parameter: Temperature

* Controls **creativity / randomness** of responses
* Typical range: **0 â†’ 2**

**Behavior by value:**

* `0 â€“ 0.5` â†’ Deterministic, predictable

  * Same input â†’ almost same output
  * Best for summaries, factual answers
* `1.5 â€“ 2` â†’ Creative, varied

  * Same input â†’ different responses
  * Best for brainstorming, idea generation

---

## ðŸ§  Types of Prompts

### 1ï¸âƒ£ Text-Based Prompts

* Most common type
* Written instructions/questions
* Main focus of this video

### 2ï¸âƒ£ Multimodal Prompts

* Include images, audio, or video
* Ask questions about non-text data
* Supported by advanced LLMs

---

## ðŸ§± Static vs Dynamic Prompts

### âŒ Static Prompts

* User writes the **entire prompt every time**
* Example:

  * "Summarize the Attention Is All You Need paper"
* Problems:

  * Too much user control
  * Inconsistent or poor outputs
  * Depends on user prompt-writing skill

### âœ… Dynamic Prompts (Recommended)

* Uses **predefined templates** with placeholders
* User only fills specific variables
* Core instruction remains controlled by developer

**Example Template:**

```
Please summarize the paper {paper_input} in a {style} and {length} format
```

**Benefits:**

* Consistent outputs
* Safer and controlled UX
* Easier to scale in applications

---

## ðŸ§© PromptTemplate Class (LangChain)

* Used to create **dynamic prompts**
* Better than Python f-strings

### Why PromptTemplate?

* âœ… **Validation**: Ensures all placeholders are filled
* â™»ï¸ **Reusability**: Can be saved as JSON and reused
* ðŸ”— **Integration**: Works seamlessly with Chains and other LangChain components

---

## ðŸ’¬ Chat History & Context Problem

### Problem

* LLMs **do not remember past messages by default**
* Example:

  * Q1: "What is 2 + 2?"
  * Q2: "Multiply it by 10"
  * âŒ LLM doesnâ€™t know what "it" refers to

### Solution: Chat History

* Maintain a list of all previous messages
* Send full conversation history with every new query

---

## ðŸ§  LangChain Message Types

LangChain defines message roles so the LLM understands context:

* **SystemMessage**

  * Sets AI behavior/persona
  * Example: "You are a helpful assistant"

* **HumanMessage**

  * Messages sent by the user

* **AIMessage**

  * Responses generated by the AI



# ChatPromptTemplate vs PromptTemplate


## ChatPromptTemplate
- Used for chat-based models
- Supports roles (system, human, ai)
- Uses MessagesPlaceholder for memory

## PromptTemplate
- Used for text generation models
- No role awareness



## 1ï¸âƒ£ What Goes **INTO** the Model vs What Comes **OUT**

### ðŸ”¹ What You **SEND** to the LLM

The input to a chat model consists of structured messages:

* **System message** â€“ sets behavior and rules
* **Human messages** â€“ user inputs
* **Previous AI messages** â€“ model replies from earlier turns (chat history)
* **Current human input** â€“ the latest question

These messages are sent together so the model understands the full context of the conversation.

---

### ðŸ”¹ What the LLM **RETURNS**

* **The next AI message**

This is the modelâ€™s response to the current conversation state.

> ðŸ‘‰ Because the AI message is the **output**, it does **not** belong inside the prompt template.

---

## 2ï¸âƒ£ Where Do Previous AI Messages Go? ðŸ¤”

### âœ… Answer: Inside `MessagesPlaceholder`

```python
MessagesPlaceholder(variable_name="chat_history")
```

`MessagesPlaceholder` is used to dynamically inject the entire conversation history into the prompt.

---

## 3ï¸âƒ£ What Does `chat_history` Contain?

Your `chat_history` list stores **both Human and AI messages**:

```python
[
  HumanMessage("Hi"),
  AIMessage("Hello!"),
  HumanMessage("What is JWT?"),
  AIMessage("JWT stands for...")
]
```

---

## 4ï¸âƒ£ How the Final Prompt Looks to the Model

When the prompt is formatted, LangChain expands it like this:

```text
SystemMessage("You are a helpful assistant")
HumanMessage("Hi")
AIMessage("Hello!")
HumanMessage("What is JWT?")
AIMessage("JWT stands for...")
HumanMessage("Explain OAuth")
```

âœ”ï¸ The model clearly sees:

* Who is **Human**
* Who is **AI**
* What the **current question** is

---

## 5ï¸âƒ£ Key Takeaway ðŸš€

* **AI messages are outputs**, not inputs
* **Do not hardcode AI messages in the template**
* **Previous AI responses live inside `MessagesPlaceholder`** as part of chat history

This design keeps conversations clean, structured, and truly multi-turn.

---

## Interview-Ready One-Liner ðŸŽ¯

> *"AI messages are generated by the model, so they are not part of the prompt template. Previous AI responses are injected via `MessagesPlaceholder` along with human messages to maintain conversation context."*
